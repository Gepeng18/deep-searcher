from typing import List, Tuple

from deepsearcher.agent.base import RAGAgent, describe_class
from deepsearcher.agent.collection_router import CollectionRouter
from deepsearcher.embedding.base import BaseEmbedding
from deepsearcher.llm.base import BaseLLM
from deepsearcher.utils import log
from deepsearcher.vector_db import RetrievalResult
from deepsearcher.vector_db.base import BaseVectorDB, deduplicate_results

# 后续查询生成提示词
# 你正在使用搜索工具通过迭代搜索数据库来回答主查询。基于以下中间查询和答案，生成一个新的简单后续问题来帮助回答主查询。
# 当之前的答案没有帮助时，你可以重新表述或分解主查询。只问简单的问题，因为搜索工具可能无法理解复杂问题。
##之前的中间查询和答案
#{intermediate_context}
##需要回答的主要查询
#{query}
#请提出一个简单的后续问题来帮助回答主查询，不要解释自己或输出其他任何内容。
FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search tool may not understand complex questions.

## Previous intermediate queries and answers
{intermediate_context}

## Main query to answer
{query}

Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.
"""

# 中间答案生成提示词
# 基于以下文档，为查询生成适当的答案。不要编造任何信息，只使用提供的文档来生成答案。如果文档不包含有用的信息，请回答"No relevant information found"。
##文档
#{retrieved_documents}
##查询
#{sub_query}
#只提供简洁的回答，不要解释自己或输出其他任何内容。
INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond "No relevant information found" if the documents do not contain useful information.

## Documents
{retrieved_documents}

## Query
{sub_query}

Respond with a concise answer only, do not explain yourself or output anything else.
"""

# 最终答案生成提示词
# 基于以下中间查询和答案，通过结合相关信息为主要查询生成最终答案。请注意，中间答案是由LLM生成的，可能并不总是准确的。
##文档
#{retrieved_documents}
##中间查询和答案
#{intermediate_context}
##主要查询
#{query}
#仅提供适当的回答，不要解释或输出任何其他内容。
FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.

## Documents
{retrieved_documents}

## Intermediate queries and answers
{intermediate_context}

## Main query
{query}

Respond with an appropriate answer only, do not explain yourself or output anything else.
"""

# 反思提示词
# 基于以下中间查询和答案，判断是否拥有足够的信息来回答主查询。如果你认为拥有足够的信息，请回答"Yes"，否则回答"No"。
# 中间查询和答案
# {intermediate_context}
#
# 主要查询
# {query}
#
# 仅回复"是"或"否"，不要解释或输出任何其他内容。
REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with "Yes", otherwise respond with "No".

## Intermediate queries and answers
{intermediate_context}

## Main query
{query}

Respond with "Yes" or "No" only, do not explain yourself or output anything else.
"""

# 获取支持文档提示词
# 基于以下文档，选择支持问答对的文档。
# 文档
# {retrieved_documents}
#
# 问答对
# 问题
# {query}
#
# 答案
# {answer}
#
# 仅回复选定文档索引的Python列表。
GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.

## Documents
{retrieved_documents}

## Q-A Pair
### Question
{query}
### Answer
{answer}

Respond with a python list of indices of the selected documents.
"""


# 检索链（Chain of Retrieval-Augmented Generation）代理实现
@describe_class(
    "This agent can decompose complex queries and gradually find the fact information of sub-queries. "
    "It is very suitable for handling concrete factual queries and multi-hop questions."
)
class ChainOfRAG(RAGAgent):
    """
    Chain of Retrieval-Augmented Generation (RAG) agent implementation.

    This agent implements a multi-step RAG process where each step can refine
    the query and retrieval process based on previous results, creating a chain
    of increasingly focused and relevant information retrieval and generation.
    Inspired by: https://arxiv.org/pdf/2501.14342

    """

    # 初始化ChainOfRAG代理
    def __init__(
        self,
        llm: BaseLLM,
        embedding_model: BaseEmbedding,
        vector_db: BaseVectorDB,
        max_iter: int = 4,
        early_stopping: bool = False,
        route_collection: bool = True,
        text_window_splitter: bool = True,
        **kwargs,
    ):
        """
        Initialize the ChainOfRAG agent with configuration parameters.

        Args:
            llm (BaseLLM): The language model to use for generating answers.
            embedding_model (BaseEmbedding): The embedding model to use for embedding queries.
            vector_db (BaseVectorDB): The vector database to search for relevant documents.
            max_iter (int, optional): The maximum number of iterations for the RAG process. Defaults to 4.
            early_stopping (bool, optional): Whether to use early stopping. Defaults to False.
            route_collection (bool, optional): Whether to route the query to specific collections. Defaults to True.
            text_window_splitter (bool, optional): Whether use text_window splitter. Defaults to True.
        """
        # 保存语言模型实例，用于生成回答和分析查询
        self.llm = llm
        # 保存嵌入模型实例，用于将查询转换为向量表示
        self.embedding_model = embedding_model
        # 保存向量数据库实例，用于存储和检索文档向量
        self.vector_db = vector_db
        # 设置最大迭代次数，控制RAG链的深度
        self.max_iter = max_iter
        # 设置是否启用早期停止，当获得足够信息时提前终止
        self.early_stopping = early_stopping
        # 设置是否启用集合路由，根据查询内容选择相关集合
        self.route_collection = route_collection
        # 初始化集合路由器，用于智能选择相关的文档集合
        self.collection_router = CollectionRouter(
            llm=self.llm, vector_db=self.vector_db, dim=embedding_model.dimension
        )
        # 设置是否使用文本窗口分割器，在检索结果中提供上下文
        self.text_window_splitter = text_window_splitter

    # 反思并生成后续子查询
    def _reflect_get_subquery(self, query: str, intermediate_context: List[str]) -> Tuple[str, int]:
        chat_response = self.llm.chat(
            [
                {
                    "role": "user",
                    "content": FOLLOWUP_QUERY_PROMPT.format(
                        query=query,
                        intermediate_context="\n".join(intermediate_context),
                    ),
                }
            ]
        )
        return self.llm.remove_think(chat_response.content), chat_response.total_tokens

    # 检索并回答
    def _retrieve_and_answer(self, query: str) -> Tuple[str, List[RetrievalResult], int]:
        consume_tokens = 0
        if self.route_collection:
            selected_collections, n_token_route = self.collection_router.invoke(
                query=query, dim=self.embedding_model.dimension
            )
        else:
            selected_collections = self.collection_router.all_collections
            n_token_route = 0
        consume_tokens += n_token_route
        all_retrieved_results = []
        for collection in selected_collections:
            log.color_print(f"<search> Search [{query}] in [{collection}]...  </search>\n")
            query_vector = self.embedding_model.embed_query(query)
            retrieved_results = self.vector_db.search_data(
                collection=collection, vector=query_vector, query_text=query
            )
            all_retrieved_results.extend(retrieved_results)
        all_retrieved_results = deduplicate_results(all_retrieved_results)
        chat_response = self.llm.chat(
            [
                {
                    "role": "user",
                    "content": INTERMEDIATE_ANSWER_PROMPT.format(
                        retrieved_documents=self._format_retrieved_results(all_retrieved_results),
                        sub_query=query,
                    ),
                }
            ]
        )
        return (
            self.llm.remove_think(chat_response.content),
            all_retrieved_results,
            consume_tokens + chat_response.total_tokens,
        )

    # 获取支持文档
    def _get_supported_docs(
        self,
        retrieved_results: List[RetrievalResult],
        query: str,
        intermediate_answer: str,
    ) -> Tuple[List[RetrievalResult], int]:
        supported_retrieved_results = []
        token_usage = 0
        if "No relevant information found" not in intermediate_answer:
            chat_response = self.llm.chat(
                [
                    {
                        "role": "user",
                        "content": GET_SUPPORTED_DOCS_PROMPT.format(
                            retrieved_documents=self._format_retrieved_results(retrieved_results),
                            query=query,
                            answer=intermediate_answer,
                        ),
                    }
                ]
            )
            supported_doc_indices = self.llm.literal_eval(chat_response.content)
            supported_retrieved_results = [
                retrieved_results[int(i)]
                for i in supported_doc_indices
                if int(i) < len(retrieved_results)
            ]
            token_usage = chat_response.total_tokens
        return supported_retrieved_results, token_usage

    # 检查是否有足够的信息
    def _check_has_enough_info(
        self, query: str, intermediate_contexts: List[str]
    ) -> Tuple[bool, int]:
        if not intermediate_contexts:
            return False, 0

        chat_response = self.llm.chat(
            [
                {
                    "role": "user",
                    "content": REFLECTION_PROMPT.format(
                        query=query,
                        intermediate_context="\n".join(intermediate_contexts),
                    ),
                }
            ]
        )
        has_enough_info = self.llm.remove_think(chat_response.content).strip().lower() == "yes"
        return has_enough_info, chat_response.total_tokens

    # 基于输入查询迭代检索相关文档
    def retrieve(self, query: str, **kwargs) -> Tuple[List[RetrievalResult], int, dict]:
        """
        Retrieves relevant documents based on the input query and iteratively refines the search.

        This method iteratively refines the search query based on intermediate results, retrieves documents,
        and filters out supported documents. It keeps track of the intermediate contexts and token usage.

        Args:
            query (str): The initial search query.
            **kwargs: Additional keyword arguments.
                - max_iter (int, optional): The maximum number of iterations for refinement. Defaults to self.max_iter.

        Returns:
            Tuple[List[RetrievalResult], int, dict]: A tuple containing:
                - List[RetrievalResult]: The list of all retrieved and deduplicated results.
                - int: The total token usage across all iterations.
                - dict: A dictionary containing additional information, including the intermediate contexts.
        """
        # 获取最大迭代次数，如果未指定则使用默认值
        max_iter = kwargs.pop("max_iter", self.max_iter)
        # 初始化中间上下文列表，用于存储每次迭代的查询和回答
        intermediate_contexts = []
        # 初始化所有检索结果的累积列表
        all_retrieved_results = []
        # 初始化token使用量计数器
        token_usage = 0
        # 开始迭代检索过程，最多进行max_iter次迭代
        for iter in range(max_iter):
            # 记录当前迭代轮次到日志
            log.color_print(f">> Iteration: {iter + 1}\n")
            # 第一步：基于当前上下文反思并生成后续查询
            followup_query, n_token0 = self._reflect_get_subquery(query, intermediate_contexts)
            # 第二步：使用后续查询进行检索并生成回答
            intermediate_answer, retrieved_results, n_token1 = self._retrieve_and_answer(
                followup_query
            )
            # 第三步：从检索结果中筛选出支持回答的文档
            supported_retrieved_results, n_token2 = self._get_supported_docs(
                retrieved_results, followup_query, intermediate_answer
            )

            # 将筛选出的支持文档添加到累积结果中
            all_retrieved_results.extend(supported_retrieved_results)
            # 计算当前中间上下文的索引编号
            intermediate_idx = len(intermediate_contexts) + 1
            # 将当前迭代的查询和回答添加到中间上下文中
            intermediate_contexts.append(
                f"Intermediate query{intermediate_idx}: {followup_query}\nIntermediate answer{intermediate_idx}: {intermediate_answer}"
            )
            # 累加本次迭代的token消耗量
            token_usage += n_token0 + n_token1 + n_token2

            # 如果启用了早期停止机制，则检查是否已有足够信息回答主查询
            if self.early_stopping:
                # 调用反思方法判断当前信息是否足够
                has_enough_info, n_token_check = self._check_has_enough_info(
                    query, intermediate_contexts
                )
                # 累加反思过程的token消耗
                token_usage += n_token_check

                # 如果已有足够信息，则提前终止迭代
                if has_enough_info:
                    # 记录早期停止信息到日志
                    log.color_print(
                        f"<think> Early stopping after iteration {iter + 1}: Have enough information to answer the main query. </think>\n"
                    )
                    # 跳出迭代循环
                    break

        # 对所有累积的检索结果进行去重处理
        all_retrieved_results = deduplicate_results(all_retrieved_results)
        # 构建返回的附加信息字典，包含所有中间上下文
        additional_info = {"intermediate_context": intermediate_contexts}
        # 返回去重后的检索结果、总token消耗量和附加信息
        return all_retrieved_results, token_usage, additional_info

    # 执行查询并返回最终答案
    def query(self, query: str, **kwargs) -> Tuple[str, List[RetrievalResult], int]:
        """
        Executes a query and returns the final answer along with all retrieved results and total token usage.

        This method initiates a query, retrieves relevant documents, and then summarizes the answer based on the retrieved documents and intermediate contexts. It logs the final answer and returns the answer content, all retrieved results, and the total token usage including the tokens used for the final answer.

        Args:
            query (str): The initial query to execute.
            **kwargs: Additional keyword arguments to pass to the `retrieve` method.

        Returns:
            Tuple[str, List[RetrievalResult], int]: A tuple containing:
                - str: The final answer content.
                - List[RetrievalResult]: The list of all retrieved and deduplicated results.
                - int: The total token usage across all iterations, including the final answer.
        """
        all_retrieved_results, n_token_retrieval, additional_info = self.retrieve(query, **kwargs)
        intermediate_context = additional_info["intermediate_context"]
        log.color_print(
            f"<think> Summarize answer from all {len(all_retrieved_results)} retrieved chunks... </think>\n"
        )
        chat_response = self.llm.chat(
            [
                {
                    "role": "user",
                    "content": FINAL_ANSWER_PROMPT.format(
                        retrieved_documents=self._format_retrieved_results(all_retrieved_results),
                        intermediate_context="\n".join(intermediate_context),
                        query=query,
                    ),
                }
            ]
        )
        log.color_print("\n==== FINAL ANSWER====\n")
        log.color_print(self.llm.remove_think(chat_response.content))
        return (
            self.llm.remove_think(chat_response.content),
            all_retrieved_results,
            n_token_retrieval + chat_response.total_tokens,
        )

    # 格式化检索结果
    def _format_retrieved_results(self, retrieved_results: List[RetrievalResult]) -> str:
        formatted_documents = []
        for i, result in enumerate(retrieved_results):
            if self.text_window_splitter and "wider_text" in result.metadata:
                text = result.metadata["wider_text"]
            else:
                text = result.text
            formatted_documents.append(f"<Document {i}>\n{text}\n<\Document {i}>")
        return "\n".join(formatted_documents)
